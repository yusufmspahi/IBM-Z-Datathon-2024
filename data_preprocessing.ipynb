{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd6991932ae8fbf",
   "metadata": {},
   "source": [
    "\n",
    "# Data Preprocessing\n",
    "In this notebook we load and process the raw data to develop the final dataset for the IBM-Z Datathon. We make use of two main datasets for a list of all observed geoeffective CMEs from the post-SOHO era between 1996-2024, and two data sets for features and targets:\n",
    "\n",
    "#### Geo-effective CMEs Targets:\n",
    "- The [Richardson and Cane list](https://izw1.caltech.edu/ACE/ASC/DATA/level3/icmetable2.htm); a list of near-Earth CMEs from 1996-2024.\n",
    "- The [George Mason University CME/ICME list](http://solar.gmu.edu/heliophysics/index.php/GMU_CME/ICME_List); a list of geoeffective CMEs from 2007-2017.\n",
    "\n",
    "#### CME Features:\n",
    "- The [SOHO-LASCO CME Catalogue](https://cdaw.gsfc.nasa.gov/CME_list/); a list of all CMEs observed from 1996-2024 containing information on physical quantities.\n",
    "- [OMNIWeb Plus data](https://omniweb.gsfc.nasa.gov/); a list of features associated with the solar wind and sunspot numbers.\n",
    "\n",
    "We will proceed by cleaning and combining the datasets to obtain and final set of features and targets. We will then explore the final dataset to make some conclusions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d11c32eddecbc3",
   "metadata": {},
   "source": "## Cleaning the data:"
  },
  {
   "cell_type": "code",
   "id": "3968ef87027f7def",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T23:16:38.729459Z",
     "start_time": "2024-10-24T23:16:38.725781Z"
    }
   },
   "source": [
    "# Importing libraries:\n",
    "# For data manipulation\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "\n",
    "# For data visualisation\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T22:23:58.364909Z",
     "start_time": "2024-10-24T22:23:58.361238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Adding filepaths as variables\n",
    "cane_file_path = r\"data/Raw Data/RichardsonCane.csv\"\n",
    "gmu_file_path = r\"data/Raw Data/GMU.csv\"\n",
    "soho_file_path = r\"data/Raw Data/SOHO_LASCO.csv\"\n",
    "omniweb_file_path = r\"data/Raw Data/OMNIWeb.csv\""
   ],
   "id": "d8febac3a4e98340",
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "id": "e65996ed00717b16",
   "metadata": {},
   "source": [
    "\n",
    "#### SOHO-LASCO Catalogue\n",
    "\n",
    "We begin by loading in the SOHO-LASCO Catalogue to obtain the physical quantities for all CMEs. The original dataset had 11 total features. Most of the data was missing for the mass and kinetic energy hence these have been excluded. We have also excluded the second-order speeds as these are correlated with the linear speed. As a result this dataset contains the dates and times for each CME, together with five features:\n",
    "- Central Position Angle in degrees.\n",
    "- Angular Width in degrees.\n",
    "- Linear Speed in km/s.\n",
    "- Acceleration in km/s$^2$.\n",
    "- Measurement Position Angle in degrees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911e643aabc4dec2",
   "metadata": {},
   "source": [
    "After inspecting the dataset, we will do the following:\n",
    "- Convert all missing values labelled as \"------\" and \"NaN\" to `None`.\n",
    "- Convert Central PA values labelled as \"Halo\" to 360.\n",
    "- Reformat the Acceleration column by removing asterisks.\n",
    "- Convert all columns to numeric.\n",
    "- Remove CME data corresponding to an angular width below 90 degrees as it is known that these are not likely to be geoeffective.\n",
    "- Replace labelled columns \"Date\" and \"Time\" with a single \"Datetime\" column."
   ]
  },
  {
   "cell_type": "code",
   "id": "8cc3bb33305d3e23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T22:23:58.459422Z",
     "start_time": "2024-10-24T22:23:58.364909Z"
    }
   },
   "source": [
    "# Reading the SOHO-LASCO Dataset\n",
    "soho_df = pd.read_csv(soho_file_path)\n",
    "\n",
    "# Replace all missing values ('------' and 'NaN') with None\n",
    "soho_df.replace(['------', 'NaN'], None, inplace=True)\n",
    "\n",
    "# Convert Angular Width values labelled as \"Halo\" to 360\n",
    "soho_df['CentralPA'] = soho_df['CentralPA'].replace('Halo', 360)\n",
    "\n",
    "# Remove asterisks from the Acceleration column\n",
    "soho_df['Accel'] = soho_df['Accel'].astype(str).str.replace('*', '', regex=False)\n",
    "\n",
    "# Convert all columns to numeric, except the first two (Date and Time)\n",
    "cols_to_convert = soho_df.columns[2:]  # Keep first two columns (Date and Time) as object\n",
    "soho_df[cols_to_convert] = soho_df[cols_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Remove rows where Central PA is below 90 degrees\n",
    "soho_df = soho_df[soho_df['AngularWidth'] >= 90]\n",
    "\n",
    "# Combine first two columns\n",
    "soho_df['Datetime'] = pd.to_datetime(soho_df['Date'] + ' ' + soho_df['Time'], dayfirst=True) # Creating Datetime column\n",
    "soho_df = soho_df.drop(soho_df.columns[[0, 1]], axis=1) # Drop the first two columns by index\n",
    "last_column = soho_df.pop(soho_df.columns[-1])  # Pop the last column\n",
    "soho_df.insert(0, last_column.name, last_column)  # Insert it at the front"
   ],
   "outputs": [],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "id": "9a7fdf56974523f6",
   "metadata": {},
   "source": [
    "#### OMNIWeb Plus Dataset\n",
    "To obtain the final list of features, we must concatenate the SOHO-LASCO Catalogue with the OMNIWeb Plus Dataset. This dataset contains 12 features associated with the solar wind:\n",
    "\n",
    "- The X-component of the magnetic field in nT.\n",
    "- The Y-component of the magnetic field in nT.\n",
    "- The Z-component of the magnetic field in nT.\n",
    "- Plasma Temperature in Kelvin.\n",
    "- Solar Proton Density n/cc.\n",
    "- Flow Speed in km/s.\n",
    "- Longitude Angle in degrees.\n",
    "- Latitude Angle in degrees.\n",
    "- Proton Density Ratio (unitless).\n",
    "- Flow Pressure in nPa.\n",
    "- Plasma Beta (unitless).\n",
    "- Sunspot Number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05a8613023db583",
   "metadata": {},
   "source": [
    "After taking a look at the OMNIWeb Plus Data, we next to the following to the dataset:\n",
    "- Reformat first column to match the SOHO-LASCO data.\n",
    "- Match times by averaging the 6-hour window after CME ejection.\n",
    "- Concatenate both datasets to obtain final list of all 17 features."
   ]
  },
  {
   "cell_type": "code",
   "id": "5e57a68e18f1f98d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T22:24:06.223283Z",
     "start_time": "2024-10-24T22:23:58.460428Z"
    }
   },
   "source": [
    "# Reading the OMNIWeb Plus Dataset\n",
    "omniweb_df = pd.read_csv(omniweb_file_path)\n",
    "\n",
    "# Reformat Datetime column\n",
    "omniweb_df['Datetime'] = pd.to_datetime(omniweb_df['Datetime'] + ':00', dayfirst=True)\n",
    "\n",
    "replacement_values = [9999, 99, 999, 999.9, 9999999., 9999., 99.99, 9.999, 999.99, 999] # Removing unavailable data\n",
    "omniweb_df.replace(replacement_values, None, inplace=True)\n",
    "\n",
    "# Define a function to get the 6-hour averaged data after each CME\n",
    "def get_solar_wind_average(cme_time, omniweb_data, window_hours=6):\n",
    "    # Get the end time for the 6-hour window\n",
    "    end_time = cme_time + pd.Timedelta(hours=window_hours)\n",
    "\n",
    "    # Filter OMNIWeb data for the 6-hour window\n",
    "    filtered_omniweb = omniweb_data[(omniweb_data['Datetime'] >= cme_time) & (omniweb_data['Datetime'] <= end_time)]\n",
    "\n",
    "    # Calculate the average of all numerical columns in this window\n",
    "    return filtered_omniweb.mean()\n",
    "\n",
    "# Apply this function to each CME in the Cane dataset\n",
    "averaged_solar_wind = soho_df['Datetime'].apply(get_solar_wind_average, omniweb_data=omniweb_df)\n",
    "\n",
    "# Combine the averaged solar wind features with the original Cane dataset\n",
    "combined_df = pd.concat([soho_df, averaged_solar_wind], axis=1)\n",
    "combined_df.columns.values[6] = 'Datetime_2' # Renaming duplicated column\n",
    "\n",
    "features_df  = combined_df.drop('Datetime_2', axis=1) # Removing duplicated Datetime column"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               Datetime  CentralPA  AngularWidth  LinearSpeed  Accel  MPA  \\\n",
       "12  1996-02-02 23:00:47        180           119         80.0    1.8  164   \n",
       "83  1996-04-29 14:38:48        360           360         65.0    NaN  149   \n",
       "85  1996-05-01 08:41:46         94            95        314.0    0.7   70   \n",
       "188 1996-06-18 17:28:50         84            95         64.0   -0.4   79   \n",
       "285 1996-07-20 09:28:16         31           175        246.0    9.4   34   \n",
       "\n",
       "           BX        BY            BZ    Plasma_Temp  Proton_Density  \\\n",
       "12   4.150000 -0.116667  3.333333e-02   75709.500000        3.466667   \n",
       "83  -0.583333  0.300000  7.833333e-01   25115.666667       11.800000   \n",
       "85   3.083333 -0.366667 -1.850372e-17   49811.000000        6.900000   \n",
       "188  0.933333 -4.950000 -1.900000e+00   60472.500000        9.083333   \n",
       "285  0.533333  3.500000 -1.283333e+00  102147.166667        7.866667   \n",
       "\n",
       "     Plasma_Speed  Plasma_Long_Angle  Plasma_Lat_Angle  Alpha_Prot_Ratio  \\\n",
       "12     493.666667           1.466667         -1.000000          0.016667   \n",
       "83     366.500000          -0.083333         -0.066667          0.012500   \n",
       "85     406.166667           2.850000         -1.916667          0.028667   \n",
       "188    388.000000          -1.900000         -2.700000          0.031667   \n",
       "285    422.500000           1.100000         -1.450000          0.019000   \n",
       "\n",
       "     Flow_Pressure  Plasma_Beta  Sunspot_No  \n",
       "12        1.506667     1.785000        12.0  \n",
       "83        2.780000    20.178333         0.0  \n",
       "85        2.116667     4.000000         0.0  \n",
       "188       2.596667     2.695000        14.0  \n",
       "285       2.518333     3.020000         0.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>CentralPA</th>\n",
       "      <th>AngularWidth</th>\n",
       "      <th>LinearSpeed</th>\n",
       "      <th>Accel</th>\n",
       "      <th>MPA</th>\n",
       "      <th>BX</th>\n",
       "      <th>BY</th>\n",
       "      <th>BZ</th>\n",
       "      <th>Plasma_Temp</th>\n",
       "      <th>Proton_Density</th>\n",
       "      <th>Plasma_Speed</th>\n",
       "      <th>Plasma_Long_Angle</th>\n",
       "      <th>Plasma_Lat_Angle</th>\n",
       "      <th>Alpha_Prot_Ratio</th>\n",
       "      <th>Flow_Pressure</th>\n",
       "      <th>Plasma_Beta</th>\n",
       "      <th>Sunspot_No</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1996-02-02 23:00:47</td>\n",
       "      <td>180</td>\n",
       "      <td>119</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>164</td>\n",
       "      <td>4.150000</td>\n",
       "      <td>-0.116667</td>\n",
       "      <td>3.333333e-02</td>\n",
       "      <td>75709.500000</td>\n",
       "      <td>3.466667</td>\n",
       "      <td>493.666667</td>\n",
       "      <td>1.466667</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>1.506667</td>\n",
       "      <td>1.785000</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1996-04-29 14:38:48</td>\n",
       "      <td>360</td>\n",
       "      <td>360</td>\n",
       "      <td>65.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>149</td>\n",
       "      <td>-0.583333</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>7.833333e-01</td>\n",
       "      <td>25115.666667</td>\n",
       "      <td>11.800000</td>\n",
       "      <td>366.500000</td>\n",
       "      <td>-0.083333</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>2.780000</td>\n",
       "      <td>20.178333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1996-05-01 08:41:46</td>\n",
       "      <td>94</td>\n",
       "      <td>95</td>\n",
       "      <td>314.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>70</td>\n",
       "      <td>3.083333</td>\n",
       "      <td>-0.366667</td>\n",
       "      <td>-1.850372e-17</td>\n",
       "      <td>49811.000000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>406.166667</td>\n",
       "      <td>2.850000</td>\n",
       "      <td>-1.916667</td>\n",
       "      <td>0.028667</td>\n",
       "      <td>2.116667</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>1996-06-18 17:28:50</td>\n",
       "      <td>84</td>\n",
       "      <td>95</td>\n",
       "      <td>64.0</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>79</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>-4.950000</td>\n",
       "      <td>-1.900000e+00</td>\n",
       "      <td>60472.500000</td>\n",
       "      <td>9.083333</td>\n",
       "      <td>388.000000</td>\n",
       "      <td>-1.900000</td>\n",
       "      <td>-2.700000</td>\n",
       "      <td>0.031667</td>\n",
       "      <td>2.596667</td>\n",
       "      <td>2.695000</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>1996-07-20 09:28:16</td>\n",
       "      <td>31</td>\n",
       "      <td>175</td>\n",
       "      <td>246.0</td>\n",
       "      <td>9.4</td>\n",
       "      <td>34</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>-1.283333e+00</td>\n",
       "      <td>102147.166667</td>\n",
       "      <td>7.866667</td>\n",
       "      <td>422.500000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>-1.450000</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>2.518333</td>\n",
       "      <td>3.020000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "id": "5b2dfdcc69b68432",
   "metadata": {},
   "source": [
    "#### Richardson and Cane Dataset\n",
    "After manually removing irrelevant columns, the Cane dataset contain three column corresponding to the targets that will be used to train our models:\n",
    "- LASCO CME Time.\n",
    "- Disturbance Time.\n",
    "- Dst Index measured in nT.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "After inspection, we will do the following:\n",
    "- Convert the first two column to the correct datetime format.\n",
    "- Remove CMEs with a Dst index greater than -30 nT and label CMEs with a Dst index of less  as Geoeffective.\n",
    "- Calculate the transit time as the difference between the LASCO CME time and the Disturbance time and convert to hours.\n"
   ],
   "id": "b89318cf679ec4f3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T22:24:06.236475Z",
     "start_time": "2024-10-24T22:24:06.223283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reading the Richardson-Cane Dataset\n",
    "cane_df = pd.read_csv(cane_file_path)\n",
    "\n",
    "# Reformatting the first two columns\n",
    "cane_df['LASCO CME Y/M/D (UT)'] = pd.to_datetime(cane_df['LASCO CME Y/M/D (UT)'], format='mixed')\n",
    "cane_df['Disturbance Y/M/D (UT)'] = pd.to_datetime(cane_df['Disturbance Y/M/D (UT)'], format='mixed')\n",
    "\n",
    "# Dropping CMEs with a Dst index greater than -30 nT\n",
    "cane_df = cane_df[cane_df['Dst (nT)'] <= -30]\n",
    "\n",
    "# Calculating transit time\n",
    "cane_df['TransitTime'] = cane_df['Disturbance Y/M/D (UT)'] - cane_df['LASCO CME Y/M/D (UT)']\n",
    "\n",
    "# Dropping CMEs with a transit time of 0\n",
    "cane_df = cane_df[cane_df['TransitTime'].dt.total_seconds() > 0]\n",
    "\n",
    "# Converting transit time to hours as a float\n",
    "cane_df['TransitTime'] = cane_df['TransitTime'].dt.total_seconds() / 3600\n",
    "\n",
    "# Labelling and creating Geoeffective column (positive class labelled as 1)\n",
    "cane_df['Geoeffective'] = 1\n",
    "cane_df = cane_df.drop(cane_df.columns[[1, 2]], axis=1)"
   ],
   "id": "2f1e1671b4d687f7",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### George Mason University Dataset\n",
    "The GMU dataset contains similar information to the Cane dataset, however, the CMEs are recorded from 2007-2017. The formatting is inconsistent so we will do the following:\n",
    "- Convert transit time column to delta-time format.\n",
    "- Concert transit time to hours as a float.\n"
   ],
   "id": "a8b06bc2a5e843be"
  },
  {
   "cell_type": "code",
   "id": "2098a7003137c35e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T22:24:06.241826Z",
     "start_time": "2024-10-24T22:24:06.236475Z"
    }
   },
   "source": [
    "# Reading the GMU dataset\n",
    "gmu_df = pd.read_csv(gmu_file_path)\n",
    "\n",
    "# Formating transit time column\n",
    "gmu_df['Transit time'] = pd.to_timedelta(gmu_df['Transit time'])\n",
    "\n",
    "# Converting transit time to hours\n",
    "gmu_df['Transit time'] = gmu_df['Transit time'].dt.total_seconds() / 3600"
   ],
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "id": "a83cbc85d4847b66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T22:24:06.244652Z",
     "start_time": "2024-10-24T22:24:06.241826Z"
    }
   },
   "source": [
    "# Saving cleaned .csv files\n",
    "#gmu_df.to_csv(r\"data/cleaned_gmu.csv\", index=False)\n",
    "#cane_df.to_csv(r\"data/cleaned_cane.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Merging all Datasets:\n",
    "To merge the features dataset with the Cane and GMU dataset we will do the following:\n",
    "- Reformat all columns to maintain consistency.\n",
    "- Concatenate the Cane and GMU datasets and remove duplicate events.\n",
    "- Set an epsilon window to iterate over each geoeffective event and match with the LASCO datetimes."
   ],
   "id": "efa3f79116aa55d3"
  },
  {
   "cell_type": "code",
   "id": "d8be5b440d539a23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T22:24:06.398148Z",
     "start_time": "2024-10-24T22:24:06.244652Z"
    }
   },
   "source": [
    "# Convert the datetime columns to a consistent format for merging\n",
    "features_df['Datetime'] = pd.to_datetime(features_df['Datetime'])\n",
    "cane_df['LASCO CME Y/M/D (UT)'] = pd.to_datetime(cane_df['LASCO CME Y/M/D (UT)'])\n",
    "gmu_df['CME in LASCO'] = pd.to_datetime(gmu_df['CME in LASCO'])\n",
    "\n",
    "# Rename columns for consistency\n",
    "cane_df.rename(columns={'LASCO CME Y/M/D (UT)': 'Datetime'}, inplace=True)\n",
    "gmu_df.rename(columns={'CME in LASCO': 'Datetime'}, inplace=True)\n",
    "\n",
    "# Merge the cleaned_cane and cleaned_gmu datasets together\n",
    "geoeffective_df = pd.concat([cane_df, gmu_df])\n",
    "\n",
    "# Remove duplicate CMEs from the combined geoeffective dataframe based on the datetime\n",
    "geoeffective_df = geoeffective_df.drop_duplicates(subset='Datetime')\n",
    "\n",
    "# Set an epsilon time window (e.g., +/- 0.5 hours)\n",
    "epsilon = timedelta(hours=0.5)\n",
    "\n",
    "# Create new columns for Geoeffective and TransitTime with default values\n",
    "merged_df = features_df.copy()\n",
    "merged_df['Geoeffective'] = 0\n",
    "merged_df['TransitTime'] = None\n",
    "\n",
    "# Iterate over each geoeffective event and match with the features within the epsilon window\n",
    "for _, row in geoeffective_df.iterrows():\n",
    "    cme_datetime = row['Datetime']\n",
    "\n",
    "    # Find matches within the epsilon window\n",
    "    mask = (merged_df['Datetime'] >= cme_datetime - epsilon) & (merged_df['Datetime'] <= cme_datetime + epsilon)\n",
    "    merged_df.loc[mask, 'Geoeffective'] = 1\n",
    "    merged_df.loc[mask, 'TransitTime'] = row['TransitTime']\n",
    "\n",
    "# Saving the final dataset as .csv\n",
    "#merged_df.to_csv(r\"data/final_dataset.csv\", index=False)"
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data Exploration and Visualisation\n",
    "Let us now explore the data to make some conclusions and discuss its potential impact. We will do the following:\n",
    "- Inspect the distribution of the positive and negative classes.\n",
    "- Inspect the distribution of the regression target.\n",
    "- Examine the impact of missing values.\n",
    "- Visualise correlation between features with respect to each ML task.\n",
    "- Inspect the feature space with dimensionality reduction techniques.\n",
    "- Analyse the uncertainty in labelling"
   ],
   "id": "28706751dc251c05"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Inspecting the Target Distributions\n",
    "We will begin by taking a look at the positive and negative classes."
   ],
   "id": "d54aae8581d33e99"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-24T23:16:53.786350Z",
     "start_time": "2024-10-24T23:16:53.782910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Counting the number of events in the positive and negative class\n",
    "geoeffective_events_count = merged_df['Geoeffective'].sum()\n",
    "nongeo_events_count = merged_df.shape[0] - geoeffective_events_count\n",
    "\n",
    "print(f\"The positive class has {geoeffective_events_count} events\")\n",
    "print(f\"The negative class has {nongeo_events_count} events\")"
   ],
   "id": "f45d1916675b8435",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The positive class has 237 events\n",
      "The negative class has 5636 events\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plotting the classes as a bar chart\n",
    "plt.style.use(['science'])\n",
    "plt.figure(figsize=[8, 5], dpi=500)\n",
    "plt.title(\"Binary Classes\", fontsize=20)\n",
    "plt.xlabel(\"$x$\", fontsize=18)\n",
    "plt.ylabel(\"$U(x)$\", fontsize=18)\n",
    "plt.grid(True)\n",
    "plt.plot(x,y)"
   ],
   "id": "2be03231b5270152"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calculate the counts of each class\n",
    "class_counts = df['Geoeffective'].value_counts()\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(6,4))\n",
    "bars = plt.bar(class_counts.index, class_counts.values, color=['blue', 'orange'])\n",
    "\n",
    "# Add labels on the bars\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval, int(yval), ha='center', va='bottom')\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Class Distribution of Geoeffective CMEs')\n",
    "plt.xlabel('Geoeffective (1 = Yes, 0 = No)')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Non-Geoeffective', 'Geoeffective'])\n",
    "plt.show()"
   ],
   "id": "84e54109cb34101c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2d984fe5dad3f714"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "aae0e37b55706238"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a84819a2af97aa99"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6b4b8f9a381cdb86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4d077cdc18ab595"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Discussion and Conclusions\n",
    "We have at last obtained the final dataset on which to train our models. There are some important things to note which are discussed as follows:\n",
    "\n",
    "<strong style=\"font-size:18px;\">Class Imbalance:</strong> One of the most prominent issues is the extreme imbalance between geoeffective and non-geoeffective CMEs. We note that only about 4% of CMEs end up impacting Earth and causing geomagnetic storms. This skewed distribution will make it difficult for the models to effectively identify the minority class.\n",
    "\n",
    "<strong style=\"font-size:18px;\">Feature Selection:</strong> The success of CME prediction models heavily relies on the selection and representation of input features. Only solar onset (solar wind) parameters have been used, which limits the modelâ€™s ability to account for CME propagation in interplanetary space.\n",
    "\n",
    "<strong style=\"font-size:18px;\">Missing Values:</strong> Our final data set suffers from missing values. Eliminating data with missing values can further reduce the already small positive class size, making it even harder to train effective models. We will deal with this in the subsequent section.\n",
    "\n",
    "<strong style=\"font-size:18px;\">Class Overlap:</strong> There is substantial similarity between the features of geoeffective and non-geoeffective CMEs, leading to overlapping data points in feature space. This class overlap makes it hard for models to distinguish between the two classes and exacerbates misclassifications.\n",
    "\n",
    "<strong style=\"font-size:18px;\">Labelling Uncertainty:</strong> Due to the uncertain association between CMEs and geomagnetic storms, the models may sometimes be trained on inaccurate data. For instance, some storms might be caused by CMEs that are difficult to link definitively to solar eruptions, leading to uncertainty in labels. Another important note is that the data does not distinguish between front-facing and back-facing CMEs (i.e. CMEs facing towards Earth or away).\n"
   ],
   "id": "3cb07aa25e613dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3fed4ac06a18d5cc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
